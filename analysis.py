# -*- coding: utf-8 -*-
"""Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SuuVmA3jcCDNBx_FW8Z01L3_ODYyRece
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pylab as pl
# %matplotlib inline
import seaborn as sb

uploaded = files.upload()

file_name = 'crisismanagement1.xlsx'

df = pd.read_excel(file_name)

print(df)

import pandas as pd

# Load the Excel file
file_name = 'crisismanagement1.xlsx'
excel_data = pd.ExcelFile(file_name)

# Print all sheet names to ensure they are correctly identified
print(excel_data.sheet_names)

# Load each sheet into a DataFrame using the exact sheet names
crisisscenario_df = pd.read_excel(file_name, sheet_name='crisisscenario')
activity_df = pd.read_excel(file_name, sheet_name='activity')
realactivity_df = pd.read_excel(file_name, sheet_name='realactivity')
actor_df = pd.read_excel(file_name, sheet_name='actor')
include_df = pd.read_excel(file_name, sheet_name='include')
apply_df = pd.read_excel(file_name, sheet_name='apply')
chasplan_df = pd.read_excel(file_name, sheet_name='chasplan')
component_df = pd.read_excel(file_name, sheet_name='component')
composition_df = pd.read_excel(file_name, sheet_name='composition')
dependence_df = pd.read_excel(file_name, sheet_name='dependence')
do_df = pd.read_excel(file_name, sheet_name='do')
feedback_df = pd.read_excel(file_name, sheet_name='feedback')
impacting_df = pd.read_excel(file_name, sheet_name='impacting')
generate_df = pd.read_excel(file_name, sheet_name='generate')
hascompo_df = pd.read_excel(file_name, sheet_name='hascompo')
occuring_df = pd.read_excel(file_name, sheet_name='occuring')
perform_df = pd.read_excel(file_name, sheet_name='perform')
plantype_df = pd.read_excel(file_name, sheet_name='plantype')
present_df = pd.read_excel(file_name, sheet_name='present')
risk_df = pd.read_excel(file_name, sheet_name='risk')
risqdammages_df = pd.read_excel(file_name, sheet_name='risqdammages')
stake_df = pd.read_excel(file_name, sheet_name='stake')
useactivity_df = pd.read_excel(file_name, sheet_name='useactivity')

# Display the first few rows of each DataFrame to understand the structure
print(crisisscenario_df.head())
print(activity_df.head())
print(realactivity_df.head())
print(actor_df.head())
print(include_df.head())
print(apply_df.head())
print(chasplan_df.head())
print(component_df.head())
print(composition_df.head())
print(dependence_df.head())
print(do_df.head())
print(feedback_df.head())
print(impacting_df.head())
print(generate_df.head())
print(hascompo_df.head())
print(occuring_df.head())
print(perform_df.head())
print(plantype_df.head())
print(present_df.head())
print(risk_df.head())
print(risqdammages_df.head())
print(stake_df.head())
print(useactivity_df.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Merge the necessary data
merged_df = pd.merge(occuring_df, risqdammages_df, on='Id_RD')
merged_df['DateOccuringRC'] = pd.to_datetime(merged_df['DateOccuringRC'])

# Map severity to numerical values
severity_mapping = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4, 'Critical': 5}
merged_df['SeverityRD'] = merged_df['SeverityRD'].map(severity_mapping)

# Aggregate data by date
time_series_df = merged_df.groupby('DateOccuringRC')['SeverityRD'].sum().reset_index()

# Plotting the data
plt.figure(figsize=(14, 7))
sns.lineplot(data=time_series_df, x='DateOccuringRC', y='SeverityRD', marker='o')
plt.title('Evolution of Damages Over Time')
plt.xlabel('Date')
plt.ylabel('Total Severity of Damages')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load the Excel file
file_name = 'crisismanagement1.xlsx'
excel_data = pd.ExcelFile(file_name)

# Load necessary sheets into DataFrames
risk_df = pd.read_excel(file_name, sheet_name='risk')

# Define the severity mapping
severity_mapping = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4, 'Critical': 5}

# Apply the severity mapping
risk_df['SeverityR'] = risk_df['SeverityR'].map(severity_mapping)

# Check for any unmapped values (those that resulted in NaN)
unmapped_values = risk_df[risk_df['SeverityR'].isna()]
if not unmapped_values.empty:
    print("Unmapped values found in SeverityR:")
    print(unmapped_values)
else:
    print("All SeverityR values mapped correctly.")

# Remove or handle unmapped values
risk_df = risk_df.dropna(subset=['SeverityR'])

# Question 2

import seaborn as sns  # Import seaborn library
import pandas as pd
import matplotlib.pyplot as plt  # Other libraries you might already have



# Calculate damage severity by date
time_series_df = merged_df.groupby('DateOccuringRC')['SeverityRD'].sum().reset_index()

# Plotting the time series data
plt.figure(figsize=(14, 7))
sns.lineplot(data=time_series_df, x='DateOccuringRC', y='SeverityRD', marker='o')
plt.title('Evolution of Damages based Risk')
plt.xlabel('Date')
plt.ylabel('Total Severity of RiskDamages')
plt.grid(True)
plt.show()

# Frequent risks
risk_counts = risk_df.groupby('TypeRisk').size().to_frame(name='Count').reset_index()
top_frequent_risks = risk_counts.sort_values(by='Count', ascending=False).head(3)

# Severe risks
most_severe_risks = risk_df.sort_values(by='SeverityR', ascending=False).head(3)

print("Top 3 Most Frequent Risks:")
print(top_frequent_risks)
print("Top 3 Most Severe Risks:")
print(most_severe_risks)

# Print results as tables (avoiding index numbers)
print("Top 3 Most Frequent Risks:")
print(top_frequent_risks.to_string(index=False))  # Avoids printing index numbers

print("\nTop 3 Most Severe Risks:")
print(most_severe_risks.to_string(index=False))

from tabulate import tabulate

# Print results as tables
print("Top 3 Most Frequent Risks:")
print(tabulate(top_frequent_risks, headers=top_frequent_risks.columns, tablefmt="grid"))

print("\nTop 3 Most Severe Risks:")
print(tabulate(most_severe_risks, headers=most_severe_risks.columns, tablefmt="grid"))